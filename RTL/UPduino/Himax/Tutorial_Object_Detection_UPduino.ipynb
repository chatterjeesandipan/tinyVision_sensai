{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tutorial_Object_Detection_UPduino.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tS65TZWkTgJN",
        "colab_type": "text"
      },
      "source": [
        "#**[tinyvision.ai](https://https://tinyvision.ai/) Object Detection Tutorial**\n",
        "---\n",
        "*Thank you for purchasing a tinyvision.ai Visual SoM development kit. We are eager to see how you put this \"tiny\" yet powerful object detector to solve everyday problems.*\n",
        "\n",
        "This tutorial focuses on training a brand-new object detection model for detecting human beings in an image stream from a camera. The process can be conveniently extended to the problem of multi-class object detection, for instance detecting different fruits in a bag. The list of objects can be expanded as per your choice. We would be utilizing the Open Images database for training and testing images in this tutorial. So, it will be a nice exercise to search the Open Images database for the object class that you might be interested in. If the object class is covered in Open Images database, then annotated training and testing images will be obtained easily. However, if your chosen object class is absent from the Open Images database, then the user of this tutorial will have to obtain and annotate the training and testing images themselves. There are a variety of methods and tools by which you can obtain training images and annotate those images, for instance the software package LabelImg. \n",
        "\n",
        "This tutorial uses Tensorflow GPU version 1.14.0. We need this version of TensorFlow in order to ensure compatibility with the Lattice software packages which will be used after this model training exercise.\n",
        "\n",
        "#Installing and loading necessary packages\n",
        "\n",
        "\n",
        "Note that Google Colab currently supports TensorFlow 2.0 but Tensorflow 1.14.0 will be utilized in this tutorial.\n",
        "\n",
        "**NOTE** After executing the next cell, you must restart your runtime in order to utilize the installed packages. If no more installations are needed, then you will be able to proceed to the next cell without any hurdle. Else, you might have to restart your runtime again. It takes a couple of seconds to restart your runtime.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Jb67IDuOwKR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt-get install tree\n",
        "!pip install -U tensorflow-gpu==1.14.0 awscli easydict numpy pandas matplotlib opencv-python tqdm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-nFcGI-_IG1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function, absolute_import\n",
        "from __future__ import division, unicode_literals\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "# %load_ext tensorboard\n",
        "\n",
        "import numpy as np\n",
        "import os, sys, glob, cv2\n",
        "from tqdm import notebook\n",
        "tqdm = notebook.tqdm\n",
        "import datetime, time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-XS1YIWISt1",
        "colab_type": "text"
      },
      "source": [
        "#**Downloading OIDv4 Toolkit**\n",
        "\n",
        "Here, we download the Github repository containing the python scripts needed for downloading the images of certain object classes from the Open Images database. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVjLajImfeZ_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Copy the Github reporsitory for Open Images Downloader Toolkit\n",
        "!git clone https://github.com/EscVM/OIDv4_ToolKit.git"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRYOxK6YIqvD",
        "colab_type": "text"
      },
      "source": [
        "#**Define the Project and Get Data**\n",
        "\n",
        "We define the project name and the objects that would be detected with the model being trained here. We choose the objects and we set the maximum number of images that we want for each class. Most of the projects will have a multiple object classes. So we will use the option of downloading all the images of all the objects in the same folder. Hence, the \"--multiclass 1\" parameter specification. \n",
        "\n",
        "**Note:** the images and labels downloaded in this step will be available in the Google Colab \tworkspace. This workspace is temporary, in the sense that the downloaded data is stored \tonly for a particular session/runtime. When the session/runtime is restarted or \tchanged, the data is lost"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QVv9WV1VNUAX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#### Case of multi-class object detection problem\n",
        "# PROJECT_NAME = \"Fruits\"\n",
        "# !python ./OIDv4_ToolKit/main.py downloader --classes Apple Orange Banana Mango --limit 800 --type_csv all --multiclass 1 -y\n",
        "\n",
        "#### Case of single class object detection problem\n",
        "PROJECT_NAME = \"Humans\"\n",
        "!python ./OIDv4_ToolKit/main.py downloader --classes Person --limit 2000 --type_csv all --multiclass 1 -y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_t859nzXJdfp",
        "colab_type": "text"
      },
      "source": [
        "**Training and Testing Data**\n",
        "\n",
        "Creating the necessary folder structure to separate the training and testing datasets. Here, we combine the training and validation images and labels into a single folder and treat it as training data, whereas all the testing data is kept for evaluating the trained object detection model. The ratio between training and testing data generally hovers around 70:30 in our examples, which is a ratio (approximately) commonly found in the machine learning literature. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVEjKha1fn97",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#### making directories to store the resized data\n",
        "BASE_PATH = \"/content/Resized_Data/\" + PROJECT_NAME\n",
        "!mkdir -p $BASE_PATH\"/training/images\"\n",
        "!mkdir -p $BASE_PATH\"/training/labels\"\n",
        "!mkdir -p $BASE_PATH\"/testing/images\"\n",
        "!mkdir -p $BASE_PATH\"/testing/labels\"\n",
        "!mkdir -p $BASE_PATH\"/ImageSets\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3wYwBL7hTWh",
        "colab_type": "code",
        "outputId": "76d72982-c630-4b4a-8184-accac5472402",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "### Find the folder name that is relevant to your project\n",
        "folders_OID = os.listdir(\"./OID/Dataset/train/\")\n",
        "\n",
        "### Choosing the Fruits dataset, hence using the word \"Apple\" to detect the right folder. Change the search keyword\n",
        "### as per your project's requirements.\n",
        "### You can replace \"Apple\" by \"Person\" to search for raw data needed for \"Human Detection Project\".\n",
        "\n",
        "### Make sure you have only one folder that contains your training data. If the following assert check fails, \n",
        "### try changing the search phrase, so that you can get the unique folder that you need for this project.\n",
        "\n",
        "folder_OID = [folder for folder in folders_OID if \"Person\" in folder.split(\"_\")]\n",
        "print(\"The folder in demand: \", folder_OID)\n",
        "\n",
        "assert len(folder_OID) == 1\n",
        "folder_OID = folder_OID[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The folder in demand:  ['Person']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0ZfSOnUKTKN",
        "colab_type": "text"
      },
      "source": [
        "#**Image and Bounding Box Resizing**\n",
        "\n",
        "In this step, we resize the images downloaded from OID to some user-defined values. Note that these image dimensions will also be used for the training algorithm. After the image resizing, we also need to ensure that the bounding box surrounding the objects in the image are resized as well. So a Python script in one of the upcoming cells does this task for each image and its corresponding label text file.\n",
        "\n",
        "**NOTE** After the resizing operation, make sure you execute the step of copying the resized training and testing images and bounding boxes to your Google Drive account. If you quit the session after resizing the images and the boxes, the data will lost from the Google Colab workspace (temporary) and you will have to start from the dowload stage described above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3wprpsfni5P5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### setting up the folders for accessing the images and labels for resizing operation\n",
        "### Make sure you are in \"/content\" directory which is equivalent to /root here.\n",
        "%cd \"/content\"\n",
        "folders = [\"train\", \"test\", \"validation\"]\n",
        "train_folder = os.path.join(\"/content/OID/Dataset/train\", folder_OID)\n",
        "test_folder = os.path.join(\"/content/OID/Dataset/test\", folder_OID)\n",
        "valid_folder = os.path.join(\"/content/OID/Dataset/validation\", folder_OID)\n",
        "\n",
        "store_train_folder = os.path.join(BASE_PATH, \"training\")\n",
        "store_test_folder = os.path.join(BASE_PATH, \"testing\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJpB4if0W9WT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# the target image size is 64 x 64 pixels. \n",
        "IMAGE_WIDTH = 64\n",
        "IMAGE_HEIGHT = 64\n",
        "TARGET_SIZE = (IMAGE_WIDTH, IMAGE_HEIGHT)\n",
        "\n",
        "\n",
        "tasks = [\"train\", \"valid\", \"test\"]\n",
        "#### add functionality to check what all actions have been completed. Start where you stopped function\n",
        "for task in tasks:\n",
        "    if task in (\"train\", \"valid\"):\n",
        "        ### Ensure that the training and validation images will be used for training, rest for testing\n",
        "        image_save_folder = os.path.join(store_train_folder, \"images\")\n",
        "        label_save_folder = os.path.join(store_train_folder, \"labels\")\n",
        "    elif task in (\"test\"):\n",
        "        image_save_folder = os.path.join(store_test_folder, \"images\")\n",
        "        label_save_folder = os.path.join(store_test_folder, \"labels\")\n",
        "\n",
        "    ## Rest of the portion of the for loop is independent of the task\n",
        "    ## (once the image/label folders have been specified)\n",
        "    \n",
        "    ### find the names (without extensions) of the image files\n",
        "    image_folder = eval(task + \"_folder\")\n",
        "    files = os.listdir(image_folder)\n",
        "    filenames = [f.split(\".\")[0] for f in files if f.endswith(\".jpg\")]\n",
        "    ### the image annotations are found using the following path\n",
        "    label_folder = os.path.join(image_folder, \"Label\")\n",
        "    \n",
        "    for i in tqdm(range(len(filenames)), desc=task.upper()):\n",
        "        ## Load the image and resize to TARGET_SIZE\n",
        "        img = cv2.imread(os.path.join(image_folder, filenames[i] + \".jpg\"))\n",
        "        height, width, channel = img.shape\n",
        "        ratio_x, ratio_y = TARGET_SIZE[0]/width, TARGET_SIZE[1]/height\n",
        "        img = cv2.resize(img, TARGET_SIZE, interpolation=cv2.INTER_AREA)\n",
        "\n",
        "        ## Save the resized image file\n",
        "        cv2.imwrite(os.path.join(image_save_folder, filenames[i] + \".jpg\"), img)\n",
        "        del img\n",
        "\n",
        "        ## Resizing the bounding box information for the above loaded image\n",
        "        with open(os.path.join(label_folder, filenames[i] + \".txt\"), mode=\"r\") as f:\n",
        "            txt = f.readlines()\n",
        "        f.close()\n",
        "        txt = [line.replace(\"\\n\", \"\") for line in txt]\n",
        "        txt_new = []\n",
        "        for line in txt:\n",
        "            line = line.split(\" \")\n",
        "            ### storing the labels in the KITTI format, as required by the squeezeDet algorithm\n",
        "            objects = [word for word in line if word.isalpha()]\n",
        "            if len(objects) > 1: \n",
        "                objects = \"_\".join(objects)\n",
        "            else:\n",
        "                objects = objects[0]\n",
        "\n",
        "            nums = [float(word) for word in line if not word.isalpha()]\n",
        "            nums[0], nums[1] = nums[0] * ratio_x, nums[1] * ratio_y\n",
        "            nums[2], nums[3] = nums[2] * ratio_x, nums[3] * ratio_y \n",
        "            nums = list(map(str, nums))\n",
        "            line = \" \".join([objects] + [\"0.00\", \"0\", \"0.00\"] + nums + [\"0.00\"]*7)\n",
        "            txt_new.append(line)\n",
        "\n",
        "        ## saving the resized bouding box numbers\n",
        "        with open(os.path.join(label_save_folder, filenames[i] + \".txt\"), mode=\"w\") as f_w:\n",
        "            for line in txt_new:\n",
        "                f_w.write(line)\n",
        "                f_w.write(\"\\n\")\n",
        "        f_w.close()\n",
        "\n",
        "### Generating image sets for training and testing\n",
        "!ls $BASE_PATH\"/training/images/\" | grep \".jpg\" | sed s/.jpg// > $BASE_PATH\"/ImageSets/train.txt\"\n",
        "!ls $BASE_PATH\"/testing/images/\" | grep \".jpg\" | sed s/.jpg// > $BASE_PATH\"/ImageSets/test.txt\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLm1Y8oGph6W",
        "colab_type": "text"
      },
      "source": [
        "#**Connect your Google Drive Account**\n",
        "\n",
        "Connect your Google drive account to store the resized image and object label files. \n",
        "\n",
        "We are not interested in storing the high resolution images (downloaded from OID) to Google drive. There are two reasons for such a decision:\n",
        "1. We do not want to fill up the free space in your Google account. Its better to store a 4 KB image than a 1 MB image. \n",
        "2. During training the images are anyways resized to a much smaller size than what we receive from OID. So it makes sense to store the images in the final (desired) dimensions directly before the training step. Now, if we start the resizing operation by repeatedly transport data between Colab and Drive, then we will observe extremely low speeds of image resizing and storage in Drive. \n",
        "\n",
        "Solution: Since the OID images and labels are present in the temporary workspace of Colab, we can resize the images and labels right here. Then, we can send the small-sized images to a specific Drive folder. That way, the image resizing happens at very high speeds. And you can store the raw data for future training purposes without cluttering up your Google account space.\n",
        "\n",
        "**NOTE**: In the upcoming cell (after Drive mount), we copy the resized data from the Colab workspace to a spacific folder in Drive. Note that the transfer of data from Colab to Drive takes time. Hence, it is recommended that after initiating the copy command to send data to the Drive folder, we need the Drive to finish storing the data which is some sense, in the pipeline between Colab and Drive. \"It has left Colab and has not yet reached Drive\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XiukfzrINKs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aaEHK0cFKlVV",
        "colab_type": "text"
      },
      "source": [
        "Creating a new folder \"TinyVsion\" in your Google Drive to store the training and testing data, as well as the Python codes needed for training. For the Python codes, we import a GitHub repository. In that repository, we have to create a folder named \"data\" (case sensitive). This newly created folder will store the (resized) training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1W4WkYTfBJ_S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DEST_PATH = \"/content/drive/My\\ Drive/TinyVision/tinyvision_ai_Object_Detection\"\n",
        "%mkdir -p $DEST_PATH\n",
        "%cd $DEST_PATH\n",
        "!git clone https://github.com/chatterjeesandipan/SqueezeDet_Quantized.git\n",
        "%mkdir -p ./SqueezeDet_Quantized/data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlUpmzsc1SNH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Copying the Resized_Data to ./squeezeDet_Quantized/data/\n",
        "%cd \"/content\"\n",
        "!rsync -recursive --progress $BASE_PATH $DEST_PATH\"/SqueezeDet_Quantized/data\"\n",
        "\n",
        "### Do NOT REMOVE the statements below:\n",
        "SECONDS_WAIT = int(20 * 60)  ### first number shows the number of minutes\n",
        "for i in tqdm(range(SECONDS_WAIT), desc=\"Uploading to Drive\"):\n",
        "    time.sleep(1)\n",
        "\n",
        "### After copying the resized data, you should see the same folder structures at the source as well as the destination folders\n",
        "### Source folder\n",
        "!tree -d \"/content/Resized_Data\"\n",
        "\n",
        "# ### Destination folder\n",
        "%cd $DEST_PATH\n",
        "!tree -d \"./SqueezeDet_Quantized/data\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YjxXhrLRBwSw",
        "colab_type": "text"
      },
      "source": [
        "#**Training Step**\n",
        "\n",
        "Here we execute the training code. The training code uses the framework developed by [Bichen Wu](https://github.com/BichenWuUCB/squeezeDet). This framework trains an object detection model that uses considerably reduced number of parameters than a typical VGG or ResNet models. The paper published by Wu *et al.* can be found [here](https://arxiv.org/abs/1612.01051).\n",
        "\n",
        "Note that the training code needs the class names for which the object detection model will be trained. Ensure that all the class names are written in lower case and separated by a blank space. The names will be processed in the training code before they are made available to the main training framework.\n",
        "\n",
        "Unfortunately in Colab, tensorboard visualization of the training process is not available because of the sequential processing of the code cells. Hence, the only option is to wait for the training process to complete and then launch a tensorboard window to visualize the graph, the loss scalars and performance on some sample training images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-zahNNFNT3y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cd $DEST_PATH\n",
        "LOG_PATH = \"./SqueezeDet_Quantized/LOGS/\" + PROJECT_NAME + \"/train\"\n",
        "DATA_PATH = \"./SqueezeDet_Quantized/data/\" + PROJECT_NAME\n",
        "\n",
        "# ### Check if you have any atleast 400 training samples in the DATA_PATH training/images directory\n",
        "assert len(os.listdir(os.path.join(DATA_PATH, \"training/images\"))) > 800,\"Too few training samples, Get more data\"\n",
        "       \n",
        "### Generating image sets for training and testing\n",
        "!ls $DATA_PATH\"/training/images/\" | grep \".jpg\" | sed s/.jpg// > $DATA_PATH\"/ImageSets/train.txt\"\n",
        "!ls $DATA_PATH\"/testing/images/\" | grep \".jpg\" | sed s/.jpg// > $DATA_PATH\"/ImageSets/test.txt\"\n",
        "\n",
        "### For the class names, look for the names you used to download training data from OIDv4.\n",
        "### In this tutorial, we have the \"Fruits\" and \"Humans\" projects.\n",
        "\n",
        "### In this example, for the fruit detection case\n",
        "# --classes = \"apple banana orange mango\" \n",
        "\n",
        "### In this example, for the human detection case\n",
        "# --classes=\"person\" \n",
        "\n",
        "!python ./SqueezeDet_Quantized/src/train.py --dataset=KITTI \\\n",
        "--data_path=$DATA_PATH --image_set=train --batch_size=128 \\\n",
        "--net=squeezeDet  --classes=\"person\" --gpu=0 \\\n",
        "--train_dir=$LOG_PATH --learning_rate=0.01 \\\n",
        "--summary_step=250 --checkpoint_step=500 --max_steps=10000 \n",
        "# --image_width=$IMAGE_WIDTH --image_height=$IMAGE_HEIGHT   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZAYFtl19g4k",
        "colab_type": "text"
      },
      "source": [
        "#**Tensorboard visualization**\n",
        "\n",
        "We visualize the model training metrics on tensorboard, along with the computation graph and some sample images on which inference is performed while training. Unfortunately, because the cells operate serially in Colab, hence visualizing the training process on tensorboard is not possible at this moment.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XLXGZGH92-mM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%load_ext tensorboard\n",
        "!tensorboard --logdir=$LOG_PATH"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "od13pAQWPkLG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### The tensorboard header mentions the code needed to stop the tensorboard visualization. Implement that code here.\n",
        "### It generally appears like the following. Note that the number (606) will be different for each tensorboard run.\n",
        "!kill 606"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOMJTbmNj0BU",
        "colab_type": "text"
      },
      "source": [
        "#**Generating Frozen Inference Graph (.pb File)**\n",
        "\n",
        "Here we use a function \"genpb.py\" to obtain the frozen infernece graph of the trained model. This gives us the .pbtxt and .pb files. After this step, follow the instructions below for downloading the Lattice softwares.....TO BE DISCUSSED LATER. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFwWLdVTkDNl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LOG_PATH = \"./SqueezeDet_Quantized/LOGS/\" + PROJECT_NAME + \"/train\"\n",
        "DATA_PATH = \"./SqueezeDet_Quantized/data/\" + PROJECT_NAME\n",
        "%cd $DEST_PATH\n",
        "\n",
        "#### Note class names must be provided in lower case\n",
        "#### if project_name is \"Humans\", then enter --classes person\n",
        "#### if project_name is \"Fruits\", then enter --classes apple banana orange mango\n",
        "\n",
        "!python ./SqueezeDet_Quantized/src/genpb.py --ckpt_dir $LOG_PATH --freeze True \\\n",
        "--classes person --image_width 64 --image_height 64 "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}